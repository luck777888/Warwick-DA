{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import math\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "content = []\n",
    "lem = []\n",
    "story_list = []\n",
    "trainToken = []\n",
    "testToken = []\n",
    "wholeToken = []\n",
    "start_time = time.time()\n",
    "\n",
    "# define a pos tag function\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#read the raw file and put every content in a list named 'content' after lowercasing\n",
    "with open('signal-news1.jsonl','r') as f:\n",
    "    for line in f.readlines():\n",
    "        d = json.loads(line)\n",
    "        content.append(d['content'].lower())\n",
    "\n",
    "for i in range(len(content)):\n",
    "    #a.1 Remove URLs e.g.\"http://www.\", \"https://www.\",\"\"\n",
    "    content[i] = re.sub(r'https?://[^\\s]+', '', content[i])\n",
    "    #a.1 Remove all non-alphanumeric characters except spaces\n",
    "    content[i] = re.sub(r'[^a-z0-9 ]+', '', content[i])\n",
    "    #a.1 Remove words with only 1 character.\n",
    "    content[i] = re.sub(r'\\b[a-z]\\b', '',content[i])\n",
    "    #a.1 Remove numbers that are fully made of digits\n",
    "    content[i] = re.sub(r'\\b[0-9]+\\b', '',content[i])\n",
    "    #a.2 Convert to tokens\n",
    "    #tokens.append(nltk.word_tokenize(content[i]))   \n",
    "    tokens = nltk.word_tokenize(content[i])\n",
    "    # tokens of first 16000 rows \n",
    "    if i < 16000:\n",
    "        for j in range(len(tokens)):\n",
    "            trainToken.append(tokens[j])\n",
    "    # tokens of rest rows\n",
    "    if i >= 16000:\n",
    "        for j in range(len(tokens)):\n",
    "            testToken.append(tokens[j])\n",
    "    # tokens of whole rows\n",
    "    for j in range(len(tokens)):\n",
    "        wholeToken.append(tokens[j])\n",
    "    #a.2 Lemmatization and pos tagging\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    story = []\n",
    "    story_dict = {}\n",
    "    for j in tag:\n",
    "        lemma = wnl.lemmatize(j[0],get_wordnet_pos(j[1]))\n",
    "        lem.append(lemma)\n",
    "        story.append(lemma)\n",
    "    story_dict['content'] = story\n",
    "    story_list.append(story_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  5690185\n",
      "V =  128765\n",
      "top 25 trigrams : [(('one', 'of', 'the'), 2428), (('on', 'share', 'of'), 2095), (('on', 'the', 'stock'), 1566), (('as', 'well', 'a'), 1415), (('in', 'research', 'report'), 1415), (('in', 'research', 'note'), 1373), (('be', 'able', 'to'), 1267), (('for', 'the', 'quarter'), 1221), (('the', 'united', 'state'), 1216), (('average', 'price', 'of'), 1193), (('research', 'report', 'on'), 1177), (('research', 'note', 'on'), 1138), (('the', 'end', 'of'), 1134), (('share', 'of', 'the'), 1133), (('in', 'report', 'on'), 1124), (('earnings', 'per', 'share'), 1119), (('cell', 'phone', 'plan'), 1073), (('phone', 'plan', 'detail'), 1070), (('accord', 'to', 'the'), 1046), (('buy', 'rating', 'to'), 1016), (('of', 'the', 'company'), 1002), (('appear', 'first', 'on'), 994), (('day', 'move', 'average'), 993), (('price', 'target', 'on'), 981), (('be', 'one', 'of'), 969)]\n",
      "the number of positive words: 176067\n",
      "the number of negative words: 142801\n",
      "the number of news stories with more positive than negative words: 10432\n",
      "the number of news stories with more negative than positive words 6881\n"
     ]
    }
   ],
   "source": [
    "#b.1 Compute N (number of tokens) and V (vocabulary size).\n",
    "# number of tokens\n",
    "N = len(lem)\n",
    "print(\"N = \",N)\n",
    "# vocabulary size\n",
    "V = len(set(lem))\n",
    "print(\"V = \",V)\n",
    "\n",
    "#b.2 List the top 25 trigrams based on the number of occurrences on the entire corpus.\n",
    "#list of trigrams\n",
    "trigrams = list(nltk.trigrams(lem))\n",
    "# the frequency of trigrams\n",
    "freq_trigrams = nltk.FreqDist(trigrams)\n",
    "#top 25 trigrams\n",
    "print(\"top 25 trigrams :\",freq_trigrams.most_common(25))\n",
    "\n",
    "#b.3 freqency of positive & negtive words\n",
    "#open positive words file\n",
    "positive_list = []\n",
    "with open('positive-words.txt','r') as positive_file:\n",
    "    for line in positive_file:\n",
    "        line=line.strip('\\n')\n",
    "        positive_list.append(line)\n",
    "positive_set = set(positive_list)\n",
    "#open negative words file\n",
    "negative_list = []\n",
    "with open('negative-words.txt','r') as negative_file:\n",
    "    for line in negative_file:\n",
    "        line=line.strip('\\n')\n",
    "        negative_list.append(line)\n",
    "negative_set = set(negative_list)\n",
    "# count positive & negative words in all 'content'\n",
    "n_positive = 0\n",
    "n_negative = 0\n",
    "for word in lem:\n",
    "    if word in positive_set:\n",
    "        n_positive += 1\n",
    "    if word in negative_set:\n",
    "        n_negative += 1\n",
    "print(\"the number of positive words:\",n_positive)\n",
    "print(\"the number of negative words:\",n_negative)    \n",
    "\n",
    "#b.4 positive story OR negative story\n",
    "n_posiStory = 0\n",
    "n_negStory = 0\n",
    "for i in range(len(story_list)):\n",
    "    n_positive = 0\n",
    "    n_negative = 0\n",
    "    for word in story_list[i]['content']:\n",
    "        # count the positive words in one content\n",
    "        if word in positive_set:\n",
    "            n_positive += 1\n",
    "        # count the negative words in one content\n",
    "        if word in negative_set:\n",
    "            n_negative += 1\n",
    "    #count the number of positive story and negative story \n",
    "    if n_positive > n_negative:\n",
    "        n_posiStory += 1\n",
    "    if n_positive < n_negative: \n",
    "        n_negStory += 1\n",
    "print(\"the number of news stories with more positive than negative words:\",n_posiStory)\n",
    "print(\"the number of news stories with more negative than positive words\",n_negStory)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'this', 'the', 'company', 'has', 'market', 'capitalization', 'of', 'billion', 'and']\n",
      "ppw = 661302.4545612346\n",
      "time =  314.2469050884247\n"
     ]
    }
   ],
   "source": [
    "#c.1 Compute language models for trigrams (first16000)\n",
    "#trigrams in training set\n",
    "train_trigrams = list(nltk.trigrams(trainToken))\n",
    "#bigram in training set\n",
    "train_bigrams = list(nltk.bigrams(trainToken))\n",
    "example_list = ['is','this']\n",
    "#produce a 10-word sentence beginning with'is' 'this'\n",
    "for j in range(8):\n",
    "    triwordlist = []\n",
    "    #find all trigrams beginning with specified bigram\n",
    "    for i in range(len(train_trigrams)):\n",
    "        if train_bigrams[i][0] == example_list[j] and train_trigrams[i][1] == example_list[j+1]:\n",
    "            triwordlist.append(train_trigrams[i])\n",
    "    #freqency of trigrams beginning with specified bigram\n",
    "    freq_exampleTrigrams = nltk.FreqDist(triwordlist)\n",
    "    #find the highest freqency of trigrams beginning with specified bigram\n",
    "    nextTrigrams = freq_exampleTrigrams.max()\n",
    "    #add the next word in the sentence\n",
    "    nextword = nextTrigrams[2]\n",
    "    example_list.append(nextword)\n",
    "print(example_list)\n",
    "\n",
    "\n",
    "#c.2 Compute the perplexity by evaluating on the remaining rows of the corpus (rows 16,001+).\n",
    "#frequency dictionary of trigrams in training set\n",
    "train_trigrams_dict = dict(nltk.FreqDist(train_trigrams))\n",
    "#frequency dictionary of bigrams in training set\n",
    "train_bigrams_dict = dict(nltk.FreqDist(train_bigrams))\n",
    "#trigrams in testing set\n",
    "test_trigrams = list(nltk.trigrams(testToken))\n",
    "#bigram in testing set\n",
    "test_bigrams = list(nltk.bigrams(testToken))    \n",
    "#bigram in whole corpus\n",
    "whole_bigrams = list(nltk.bigrams(wholeToken)) \n",
    "#the size of whole corpus bigrams\n",
    "V_bi = len(set(whole_bigrams))\n",
    "#number of trigrams\n",
    "N_tri = len(test_trigrams)\n",
    "sum_logp = 0 \n",
    "for i in range(len(test_trigrams)):\n",
    "    if test_trigrams[i] in train_trigrams_dict:\n",
    "        c_tri = train_trigrams_dict[test_trigrams[i]]\n",
    "    else:\n",
    "        c_tri = 0\n",
    "    if test_trigrams[i][:2] in train_bigrams_dict:\n",
    "        c_bi = train_bigrams_dict[test_trigrams[i][:2]]\n",
    "    else:\n",
    "        c_bi = 0\n",
    "    # Laplace Smoothing add-one estimate\n",
    "    p = (c_tri + 1) / (c_bi + V_bi)\n",
    "    logp = math.log(p)\n",
    "    sum_logp += logp\n",
    "# compute the perplexity \n",
    "ppw = math.exp(-(sum_logp / (N_tri)))\n",
    "print(\"ppw =\",ppw)\n",
    "# running time\n",
    "end_time = time.time()    \n",
    "print(\"time = \",end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
